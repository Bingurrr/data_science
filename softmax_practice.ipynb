{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20181612_박병규_과제3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P2FmAjz5yer"
      },
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0],\n",
        " [1,0,0], [1,0,0] ])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZjbwOhwKkSW",
        "outputId": "97ef83fe-1856-4259-82a0-8a5139e9d883"
      },
      "source": [
        "## W,b 초기화\n",
        "## Optimizer 생성\n",
        "\n",
        "W = torch.zeros(4, 3, requires_grad=True)\n",
        "b = torch.zeros(1, 3, requires_grad=True)\n",
        "optimizer = torch.optim.Adam([W,b], lr=0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "  hypothesis = torch.softmax(torch.mm(x_train, W)+b, dim=1)\n",
        "  cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim=1))\n",
        "  hypothesis = (torch.mm(x_train, W)+b).softmax(dim=1)\n",
        "  cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "  if epoch % 300 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.098612\n",
            "epoch: 300, cost: 0.105262\n",
            "epoch: 600, cost: 0.042634\n",
            "epoch: 900, cost: 0.023111\n",
            "epoch: 1200, cost: 0.014479\n",
            "epoch: 1500, cost: 0.009879\n",
            "epoch: 1800, cost: 0.007124\n",
            "epoch: 2100, cost: 0.005338\n",
            "epoch: 2400, cost: 0.004113\n",
            "epoch: 2700, cost: 0.003236\n",
            "epoch: 3000, cost: 0.002588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw4JirMBKrPp",
        "outputId": "cb8231c0-da3d-4711-c45a-78f97a38b835"
      },
      "source": [
        "W.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "\n",
        "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "test_all = torch.softmax(torch.mm(x_test, W)+b, dim=1)\n",
        "\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 5.5165e-19, 7.0151e-38],\n",
            "        [1.4800e-02, 7.4294e-01, 2.4226e-01],\n",
            "        [1.2256e-33, 9.0835e-12, 1.0000e+00]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVKvg49PSAtr"
      },
      "source": [
        "조금 더 깔끔하게 Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfd0ZuqCRzEv"
      },
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.LongTensor([ 2, 2, 2, 1, 1, 1, 0, 0 ])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ArUSlUKtjF0",
        "outputId": "ba98364e-62ee-4280-d5c5-5852d18c46fd"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "W = torch.zeros(4, 3, requires_grad=True)\n",
        "b = torch.zeros(1, 3, requires_grad=True)\n",
        "optimizer = torch.optim.Adam([W,b], lr=0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        " z = torch.mm(x_train, W)+b\n",
        " cost = F.cross_entropy(z, y_train)\n",
        " \n",
        " optimizer.zero_grad()\n",
        " cost.backward()\n",
        " optimizer.step()\n",
        "\n",
        " if epoch % 300 == 0:\n",
        "  print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.098612\n",
            "epoch: 300, cost: 0.105263\n",
            "epoch: 600, cost: 0.042634\n",
            "epoch: 900, cost: 0.023111\n",
            "epoch: 1200, cost: 0.014479\n",
            "epoch: 1500, cost: 0.009879\n",
            "epoch: 1800, cost: 0.007124\n",
            "epoch: 2100, cost: 0.005338\n",
            "epoch: 2400, cost: 0.004113\n",
            "epoch: 2700, cost: 0.003236\n",
            "epoch: 3000, cost: 0.002588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao421S_pY7ZP",
        "outputId": "2e32a6c4-c485-4e48-c960-d3267808a4df"
      },
      "source": [
        "W.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "\n",
        "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "test_all = torch.mm(x_test, W)+b\n",
        "\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 39.3535,  -2.6879, -46.1967],\n",
            "        [ -5.5873,  -1.6713,  -2.7919],\n",
            "        [-44.3165,   6.0408,  31.4654]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhKEMAegcWdK"
      },
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.LongTensor([ 2, 2, 2, 1, 1, 1, 0, 0 ])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsVLGpLmRzCr",
        "outputId": "8b7ddba3-29c0-4258-ad03-df1d4638adff"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Linear(4,3)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "  z = model(x_train)\n",
        "  cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 300 == 0:\n",
        "    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 2.127651\n",
            "epoch: 300, cost: 0.032930\n",
            "epoch: 600, cost: 0.013113\n",
            "epoch: 900, cost: 0.007087\n",
            "epoch: 1200, cost: 0.004445\n",
            "epoch: 1500, cost: 0.003039\n",
            "epoch: 1800, cost: 0.002196\n",
            "epoch: 2100, cost: 0.001648\n",
            "epoch: 2400, cost: 0.001272\n",
            "epoch: 2700, cost: 0.001002\n",
            "epoch: 3000, cost: 0.000802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WiBMLv6duU5",
        "outputId": "f4ba7797-7c9b-4faf-a65f-55093af3f6b0"
      },
      "source": [
        "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "test_all = model(x_test)\n",
        "\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 68.7222,  14.6927, -39.6412],\n",
            "        [ -1.8296,  10.3546,   7.4684],\n",
            "        [-50.0765,  15.4791,  46.8552]], grad_fn=<AddmmBackward>)\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ3PWk8ZWl0g"
      },
      "source": [
        "Softmax Regression in Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrR7BzAcWk9J",
        "outputId": "2d1268d5-c1df-4b60-d106-23dd253e23fc"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "x_train = np.array([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "\n",
        "# y에 0, 1, 2 등 둘 이상의 class가 존재 => softmax regression\n",
        "y_train = np.array([ 2, 2, 2, 1, 1, 1, 0, 0 ])\n",
        "\n",
        "logistic = LogisticRegression() # 모델 생성\n",
        "logistic.fit(x_train, y_train) # 학습\n",
        "pred = logistic.predict([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) # test case (값 예측)\n",
        "print(pred) # 출력"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 2]\n"
          ]
        }
      ]
    }
  ]
}